{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata":gemini-1.5-flash-8b
   "source": [gemini-1.5-flash-8b
    "# Importar Bibliotecas Necess√°rias\n",
    "Importar gemini-1.5-flash-8bess√°rias, incluindo os m√≥dulos para carregar vari√°veis de ambiente e a biblioteca generativeai do Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},gemini-1.5-flash-8b
   "outputs": gemini-1.5-flash-8b
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anacgemini-1.5-flash-8bnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],gemini-1.5-flash-8b
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",gemini-1.5-flash-8b
    "# Carregar vari√°veis de ambiente do arquivo .env\n",
    "load_dotegemini-1.5-flash-8b
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurar o Modelo Generativo\n",
    "Configurar o modelo generativo com a chave de API e as configura√ß√µes b√°sicas de gera√ß√£o."
   ]
  },
  {gemini-1.5-flash-8b
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°! Estou √≥timo, obrigado por perguntar. Como posso te ajudar hoje?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configurar a chave de API e as configura√ß√µes b√°sicas de gera√ß√£o\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Configura√ß√µes de gera√ß√£o\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Criar o modelo generativo\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um chatbot. Responda √†s mensagens do usu√°rio de maneira √∫til e amig√°vel.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta\n",
    "response = chat_session.send_message(\"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√©cnica 1: Ajuste de Temperatura\n",
    "Demonstrar como ajustar a temperatura para controlar a aleatoriedade das respostas geradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com temperatura 0.2: Estou bem, obrigado por perguntar! Como posso te ajudar hoje?\n",
      "\n",
      "Resposta com temperatura 0.8: Estou bem, obrigado por perguntar! Como posso te ajudar hoje?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ajustar a temperatura para um valor mais baixo para respostas mais previs√≠veis\n",
    "generation_config[\"temperature\"] = 0.2\n",
    "\n",
    "# Atualizar o modelo com a nova configura√ß√£o de temperatura\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um chatbot. Responda √†s mensagens do usu√°rio de maneira √∫til e amig√°vel.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta com a nova configura√ß√£o de temperatura\n",
    "response = chat_session.send_message(\"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com temperatura 0.2:\", response.text)\n",
    "\n",
    "# Ajustar a temperatura para um valor mais alto para respostas mais criativas\n",
    "generation_config[\"temperature\"] = 0.8\n",
    "\n",
    "# Atualizar o modelo com a nova configura√ß√£o de temperatura\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um chatbot. Responda √†s mensagens do usu√°rio de maneira √∫til e amig√°vel.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta com a nova configura√ß√£o de temperatura\n",
    "response = chat_session.send_message(\"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com temperatura 0.8:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√©cnica 2: Ajuste de Top-p e Top-k\n",
    "Mostrar como ajustar os par√¢metros top-p e top-k para controlar a diversidade das respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com config {'temperature': 1, 'top_p': 0.7, 'top_k': 40, 'max_output_tokens': 8192}: Estou bem, obrigado por perguntar! Como posso ajud√°-lo hoje?\n",
      "\n",
      "Resposta com config {'temperature': 1, 'top_p': 0.95, 'top_k': 40, 'max_output_tokens': 8192}: Estou bem, obrigado por perguntar! Como voc√™ est√° hoje?\n",
      "Erro: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Carregar vari√°veis de ambiente do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Fun√ß√£o para enviar mensagem e imprimir resposta\n",
    "def send_message_and_print_response(config, message):\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-1.5-flash-8b\",\n",
    "            generation_config=config,\n",
    "            system_instruction=\"Voc√™ √© um chatbot. Responda √†s mensagens do usu√°rio de maneira √∫til e amig√°vel.\",\n",
    "        )\n",
    "        chat_session = model.start_chat(history=[])\n",
    "        response = chat_session.send_message(message)\n",
    "        print(f\"Resposta com config {config}: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "\n",
    "# Configura√ß√£o inicial\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "# Ajustar top_p para um valor mais baixo para respostas mais focadas\n",
    "generation_config[\"top_p\"] = 0.7\n",
    "send_message_and_print_response(generation_config, \"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Ajustar top_p para um valor mais alto para respostas mais diversas\n",
    "generation_config[\"top_p\"] = 0.95\n",
    "send_message_and_print_response(generation_config, \"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Ajustar top_k para um valor mais baixo para respostas mais focadas\n",
    "generation_config[\"top_k\"] = 20\n",
    "send_message_and_print_response(generation_config, \"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Ajustar top_k para um valor mais alto para respostas mais diversas\n",
    "generation_config[\"top_k\"] = 50\n",
    "send_message_and_print_response(generation_config, \"Ol√°, como voc√™ est√°?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√©cnica 3: Instru√ß√µes do Sistema\n",
    "Explicar como fornecer instru√ß√µes espec√≠ficas ao sistema para guiar o comportamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com instru√ß√£o formal: Estou bem, obrigado por perguntar. Como posso ajud√°-lo hoje?\n",
      "\n",
      "Resposta com instru√ß√£o casual: E a√≠, tudo bem? T√¥ de boas por aqui, e voc√™? Conta as novidades! üòÑ\n",
      "\n",
      "Resposta com instru√ß√£o t√©cnica: O aprendizado de m√°quina √© um subcampo da intelig√™ncia artificial (IA) que permite aos computadores aprender com dados sem serem explicitamente programados. Em vez de depender de regras e instru√ß√µes codificadas manualmente, os algoritmos de aprendizado de m√°quina identificam padr√µes, fazem previs√µes e melhoram seu desempenho ao longo do tempo por meio da exposi√ß√£o a dados.\n",
      "\n",
      "Aqui est√° uma an√°lise de como funciona o aprendizado de m√°quina:\n",
      "\n",
      "**Componentes principais:**\n",
      "\n",
      "1. **Dados:** O aprendizado de m√°quina requer dados para aprender. Os dados podem ser qualquer coisa, desde imagens e texto at√© n√∫meros e medi√ß√µes de sensores. A qualidade e a quantidade dos dados s√£o cruciais para o sucesso de um modelo de aprendizado de m√°quina. Mais dados geralmente levam a melhores resultados.\n",
      "\n",
      "2. **Modelo:** Um modelo √© uma representa√ß√£o matem√°tica ou estat√≠stica de um sistema ou processo do mundo real. Os algoritmos de aprendizado de m√°quina usam dados para treinar um modelo, ajustando seus par√¢metros at√© que ele possa fazer previs√µes precisas.\n",
      "\n",
      "3. **Algoritmo:** Um algoritmo √© um conjunto de regras e c√°lculos que um modelo usa para aprender com os dados. Diferentes algoritmos s√£o adequados para diferentes tipos de problemas e dados. A escolha do algoritmo certo √© essencial para o sucesso de um projeto de aprendizado de m√°quina.\n",
      "\n",
      "4. **Previs√µes/Decis√µes:** Depois que um modelo √© treinado, ele pode ser usado para fazer previs√µes ou tomar decis√µes sobre novos dados. A precis√£o dessas previs√µes depende da qualidade dos dados e do algoritmo usado.\n",
      "\n",
      "**Processo de aprendizado de m√°quina:**\n",
      "\n",
      "1. **Coleta e prepara√ß√£o de dados:** O primeiro passo √© coletar e preparar os dados que ser√£o usados para treinar o modelo. Isso inclui a limpeza dos dados, o tratamento de valores ausentes e a convers√£o dos dados em um formato que o algoritmo possa entender.\n",
      "\n",
      "2. **Escolha de um algoritmo:** O pr√≥ximo passo √© escolher um algoritmo apropriado para o problema e os dados. Existem muitos algoritmos diferentes de aprendizado de m√°quina dispon√≠veis, cada um com seus pr√≥prios pontos fortes e fracos.\n",
      "\n",
      "3. **Treinamento do modelo:** O algoritmo √© ent√£o usado para treinar o modelo nos dados preparados. Isso envolve alimentar os dados no algoritmo e permitir que ele ajuste os par√¢metros do modelo at√© que ele possa fazer previs√µes precisas.\n",
      "\n",
      "4. **Avalia√ß√£o do modelo:** Depois que o modelo √© treinado, ele √© avaliado em um conjunto separado de dados para avaliar seu desempenho. Isso ajuda a garantir que o modelo n√£o esteja superajustado aos dados de treinamento e que pode generalizar para novos dados.\n",
      "\n",
      "5. **Ajuste do modelo:** Com base nos resultados da avalia√ß√£o, o modelo pode ser ajustado ainda mais para melhorar seu desempenho. Isso pode envolver a altera√ß√£o dos par√¢metros do modelo ou o uso de um algoritmo diferente.\n",
      "\n",
      "6. **Previs√£o/Tomada de decis√£o:** Finalmente, o modelo treinado pode ser usado para fazer previs√µes ou tomar decis√µes sobre novos dados.\n",
      "\n",
      "**Tipos de aprendizado de m√°quina:**\n",
      "\n",
      "* **Aprendizado supervisionado:** O modelo √© treinado em dados rotulados, o que significa que a sa√≠da desejada √© fornecida para cada entrada. O objetivo √© aprender uma fun√ß√£o de mapeamento que pode prever a sa√≠da para novas entradas.\n",
      "\n",
      "* **Aprendizado n√£o supervisionado:** O modelo √© treinado em dados n√£o rotulados, o que significa que nenhuma sa√≠da desejada √© fornecida. O objetivo √© descobrir padr√µes e estruturas nos dados sem qualquer orienta√ß√£o expl√≠cita.\n",
      "\n",
      "* **Aprendizado por refor√ßo:** O modelo aprende interagindo com um ambiente. Ele recebe recompensas ou penalidades por suas a√ß√µes e aprende a maximizar sua recompensa ao longo do tempo.\n",
      "\n",
      "**Exemplos de aplica√ß√µes de aprendizado de m√°quina:**\n",
      "\n",
      "* **Reconhecimento de imagem:** Identifica√ß√£o de objetos, rostos e outras caracter√≠sticas em imagens.\n",
      "* **Processamento de linguagem natural:** An√°lise e compreens√£o de texto humano.\n",
      "* **Detec√ß√£o de fraudes:** Identifica√ß√£o de transa√ß√µes fraudulentas.\n",
      "* **Sistemas de recomenda√ß√£o:** Recomenda√ß√£o de produtos ou servi√ßos aos usu√°rios.\n",
      "* **Carros aut√¥nomos:** Navega√ß√£o e controle de ve√≠culos sem interven√ß√£o humana.\n",
      "\n",
      "Este √© um resumo de como funciona o aprendizado de m√°quina. √â um campo complexo e em r√°pida evolu√ß√£o, mas os princ√≠pios b√°sicos permanecem os mesmos. Ao entender esses princ√≠pios, voc√™ pode come√ßar a apreciar o poder e o potencial do aprendizado de m√°quina.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# T√©cnica 3: Instru√ß√µes do Sistema\n",
    "\n",
    "# Atualizar a instru√ß√£o do sistema para um comportamento mais formal\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um assistente virtual formal. Responda √†s mensagens do usu√°rio de maneira educada e profissional.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta com a nova instru√ß√£o do sistema\n",
    "response = chat_session.send_message(\"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com instru√ß√£o formal:\", response.text)\n",
    "\n",
    "# Atualizar a instru√ß√£o do sistema para um comportamento mais casual\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um amigo virtual. Responda √†s mensagens do usu√°rio de maneira descontra√≠da e amig√°vel.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta com a nova instru√ß√£o do sistema\n",
    "response = chat_session.send_message(\"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com instru√ß√£o casual:\", response.text)\n",
    "\n",
    "# Atualizar a instru√ß√£o do sistema para fornecer respostas detalhadas\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um assistente t√©cnico. Forne√ßa respostas detalhadas e informativas √†s perguntas do usu√°rio.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta com a nova instru√ß√£o do sistema\n",
    "response = chat_session.send_message(\"Como funciona o aprendizado de m√°quina?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com instru√ß√£o t√©cnica:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√©cnica 4: Hist√≥rico de Conversa\n",
    "Demonstrar como utilizar o hist√≥rico de conversa para manter o contexto entre as intera√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com hist√≥rico de conversa: Claro! Aqui vai uma:\n",
      "\n",
      "Por que o fantasma n√£o consegue terminar sua li√ß√£o de casa?\n",
      "\n",
      "Porque ele continua passando por ela!\n",
      "\n",
      "Gostaria de ouvir outra piada?\n",
      "\n",
      "Resposta com hist√≥rico de conversa atualizado: Desculpe, n√£o tenho acesso a informa√ß√µes em tempo real, como a previs√£o do tempo. Um aplicativo de previs√£o do tempo ou o seu servi√ßo de meteorologia local seriam os melhores recursos para obter informa√ß√µes atualizadas. Voc√™ pode me dizer sua localiza√ß√£o e eu posso ajudar voc√™ a encontrar os recursos meteorol√≥gicos locais relevantes?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Carregar vari√°veis de ambiente do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Configura√ß√£o inicial\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "# Criar o modelo\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash-8b\",\n",
    "    generation_config=generation_config,\n",
    "    system_instruction=\"Voc√™ √© um chatbot. Responda √†s mensagens do usu√°rio de maneira √∫til e amig√°vel.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat com hist√≥rico\n",
    "chat_session = model.start_chat(\n",
    "    history=[\n",
    "        {\"parts\": [{\"text\": \"Ol√°, como voc√™ est√°?\"}], \"role\": \"user\"},\n",
    "        {\"parts\": [{\"text\": \"Estou bem, obrigado! Como posso ajudar voc√™ hoje?\"}], \"role\": \"assistant\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Enviar uma nova mensagem e obter a resposta mantendo o contexto\n",
    "response = chat_session.send_message(\"Pode me contar uma piada?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com hist√≥rico de conversa:\", response.text)\n",
    "\n",
    "# Adicionar mais intera√ß√µes ao hist√≥rico\n",
    "chat_session.history.append({\"parts\": [{\"text\": \"Qual √© a previs√£o do tempo para hoje?\"}], \"role\": \"user\"})\n",
    "\n",
    "# Enviar outra mensagem e obter a resposta mantendo o contexto\n",
    "response = chat_session.send_message(\"E amanh√£?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com hist√≥rico de conversa atualizado:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√©cnica 5: Manipula√ß√£o de Tokens de Sa√≠da\n",
    "Mostrar como manipular o n√∫mero m√°ximo de tokens de sa√≠da para controlar o comprimento das respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta com max_output_tokens 50: Ol√°! Estou √≥timo e pronto para ajudar. O que posso fazer por voc√™ hoje?\n",
      "\n",
      "Resposta com max_output_tokens 200: Estou bem, obrigado por perguntar! Como vai voc√™? Espero que esteja tendo um dia maravilhoso. No que posso ajudar voc√™ hoje?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# T√©cnica 5: Manipula√ß√£o de Tokens de Sa√≠da\n",
    "\n",
    "# Ajustar o n√∫mero m√°ximo de tokens de sa√≠da para um valor mais baixo para respostas mais curtas\n",
    "generation_config[\"max_output_tokens\"] = 50\n",
    "\n",
    "# Atualizar o modelo com a nova configura√ß√£o de max_output_tokens\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um chatbot. Responda √†s mensagens do usu√°rio de maneira √∫til e amig√°vel.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta com a nova configura√ß√£o de max_output_tokens\n",
    "response = chat_session.send_message(\"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com max_output_tokens 50:\", response.text)\n",
    "\n",
    "# Ajustar o n√∫mero m√°ximo de tokens de sa√≠da para um valor mais alto para respostas mais longas\n",
    "generation_config[\"max_output_tokens\"] = 200\n",
    "\n",
    "# Atualizar o modelo com a nova configura√ß√£o de max_output_tokens\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash-8b\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"Voc√™ √© um chatbot. Responda √†s mensagens do usu√°rio de maneira √∫til e amig√°vel.\",\n",
    ")\n",
    "\n",
    "# Iniciar uma nova sess√£o de chat\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Enviar uma mensagem e obter a resposta com a nova configura√ß√£o de max_output_tokens\n",
    "response = chat_session.send_message(\"Ol√°, como voc√™ est√°?\")\n",
    "\n",
    "# Imprimir a resposta\n",
    "print(\"Resposta com max_output_tokens 200:\", response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
